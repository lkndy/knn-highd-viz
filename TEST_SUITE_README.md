# HKNN Test Suite

Comprehensive test suite for analyzing HKNN algorithm behavior for paper research.

## Overview

The test suite consists of three main components:

1. **`test_suite.py`** - Main test runner that executes multiple configurations
2. **`collect_metrics.py`** - Collects comprehensive metrics from embeddings
3. **`results_analyzer.py`** - Analyzes and visualizes test results

## Quick Start

### 1. Run a Baseline Test

```bash
# Generate test data first
uv run python generate_test_data.py --N 1000 --output test_mnist

# Run baseline test
uv run python test_suite.py \
  --data test_mnist.fvecs \
  --N 1000 \
  --D 784 \
  --test-type baseline \
  --output-dir test_results
```

### 2. Run Parameter Sweeps

```bash
# Sweep over K values
uv run python test_suite.py \
  --data test_mnist.fvecs \
  --N 1000 \
  --D 784 \
  --test-type K_sweep \
  --output-dir test_results

# Sweep over learning rates
uv run python test_suite.py \
  --data test_mnist.fvecs \
  --N 1000 \
  --D 784 \
  --test-type lr_sweep \
  --output-dir test_results

# Comprehensive test (all parameter sweeps)
uv run python test_suite.py \
  --data test_mnist.fvecs \
  --N 1000 \
  --D 784 \
  --test-type comprehensive \
  --output-dir test_results \
  --max-tests 20  # Limit for quick testing
```

### 3. Analyze Results

```bash
# Generate analysis report and plots
uv run python results_analyzer.py \
  --results-dir test_results/run_YYYYMMDD_HHMMSS \
  --generate-plots
```

## Test Types

- **`baseline`** - Single run with default parameters
- **`K_sweep`** - Vary K (nearest neighbors): 30, 50, 100, 150
- **`perplexity_sweep`** - Vary perplexity: 30, 50, 75, 100
- **`lr_sweep`** - Vary learning rate: 50, 100, 200, 400
- **`epochs_sweep`** - Vary epochs (coarse × fine combinations)
- **`mneg_sweep`** - Vary negative samples: 3, 5, 10
- **`gamma_sweep`** - Vary gamma: 1.0, 3.0, 5.0, 7.0
- **`coarsening_sweep`** - Vary coarsening parameters (kml, rho)
- **`comprehensive`** - Run all single-parameter sweeps

## Output Structure

```
test_results/
└── run_YYYYMMDD_HHMMSS/
    ├── test_config.json          # Test configuration
    ├── results.json              # Full results (JSON)
    ├── results_summary.csv       # Summary CSV for analysis
    ├── metrics_*.json            # Individual metric files
    ├── test_*.csv                # Embedding outputs
    └── analysis/                 # Generated by results_analyzer.py
        ├── summary_report.txt    # Text summary
        ├── detailed_results.csv  # Full results CSV
        └── plots/                # Parameter sensitivity plots
            ├── K_knn_overlap_pct.png
            ├── lr_elapsed_time.png
            └── ...
```

## Metrics Collected

### Performance Metrics
- `elapsed_time` - Runtime in seconds

### Quality Metrics
- `knn_overlap_pct` - KNN preservation percentage
- `knn_overlap_mean` - Mean overlap with original KNN
- `silhouette_*` - Cluster quality (for different cluster counts)
- `duplicate_pct` - Percentage of duplicate coordinates
- `unique_coords` - Number of unique coordinate pairs

### Structural Metrics
- `x_span`, `y_span` - Embedding spread
- `x_std`, `y_std` - Standard deviations
- `min_dist_mean` - Mean minimum distance to neighbors
- `local_density_mean` - Mean local point density
- `cluster_imbalance_*` - Cluster size imbalance

### Ground Truth Metrics (if available)
- `ari_score` - Adjusted Rand Index (for discrete labels)
- `spatial_correlation` - Spearman correlation (for continuous labels)

## Usage Examples

### Parameter Sensitivity Analysis

```bash
# Test K parameter
uv run python test_suite.py \
  --data test_mnist.fvecs --N 1000 --D 784 \
  --test-type K_sweep \
  --output-dir results

# Analyze results
uv run python results_analyzer.py \
  --results-dir results/run_* \
  --generate-plots
```

### Quick Testing with Limited Runs

```bash
# Run only 5 tests
uv run python test_suite.py \
  --data test_mnist.fvecs --N 1000 --D 784 \
  --test-type comprehensive \
  --max-tests 5 \
  --output-dir quick_test
```

### Analyze Existing Results

```bash
# Skip embedding generation, only analyze
uv run python test_suite.py \
  --data test_mnist.fvecs --N 1000 --D 784 \
  --test-type baseline \
  --skip-embedding \
  --output-dir existing_results
```

## Results Analysis

The `results_analyzer.py` script generates:

1. **Summary Report** (`summary_report.txt`):
   - Overall statistics
   - Performance metrics
   - Quality metrics
   - Parameter sensitivity analysis
   - Best configurations

2. **Plots** (if `--generate-plots`):
   - Parameter vs metric scatter plots
   - Trend lines
   - All combinations of parameters and metrics

3. **CSV Export**:
   - `detailed_results.csv` - Full results for custom analysis

## Custom Test Configurations

Edit `test_suite.py` to modify:
- Parameter ranges in `TEST_CONFIGS`
- Test configurations in `generate_test_configs()`
- Additional metrics in `collect_metrics.py`

## Tips for Paper Analysis

1. **Start with baseline** to establish reference performance
2. **Run parameter sweeps** one at a time for focused analysis
3. **Use `--max-tests`** for quick iterations during development
4. **Check `summary_report.txt`** for quick insights
5. **Use plots** for visual parameter sensitivity analysis
6. **Export CSV** for statistical analysis in R/Python/Excel

## Dependencies

All dependencies are managed via `uv`:
- numpy, pandas, scipy, scikit-learn
- matplotlib, seaborn (for plotting)

Install with: `uv sync`

